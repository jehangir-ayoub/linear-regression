# -*- coding: utf-8 -*-
"""linear regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KfYPzKcx9soV7NN4O2QyaCMHMFRra8pj

#linear regression
y=wx+B
y = dependenet variable
x = independent variable
w =weight
b =bias
#gradient descent
it is an optimization technique used for minimizing the loss function in various machine learning algorithms it i sused for updating the parameters of the learning model
w =w-a*dw
b=b-a*db      a here is alpha which is learning rate  and dw is partial derivative  w is slope and b is intercept
#learning rate
it is a tuning parameter in an optimization algo that determines the step size at each iteration while moving toward a minimum of a loss fxn

dw=-2/n{xi(yi-ypred)   #{summation

db=-2/n{(yi-ypred)
"""

import numpy as np

#linear regression what is self
#initiating the parameters which is learning arte and no of iterations
class LinearRegression:
  def __init__(self ,learning_rate ,no_of_iterations):   #hyper parameters
    self.learning_rate = learning_rate
    self.no_of_iterations = no_of_iterations

  def fit(self,X,Y):

    # no of training examples(m  which is rows 30) and features(n no of coulmns which is 1)
    self.m,self.n = X.shape

    #initiating the weight and bias
    self.w = np.zeros(self.n)
    self.b = 0
    self.X = X
    self.Y = Y



    # implementing gradient descent
    for i in range(self.no_of_iterations):
      self.update_weights()



  def update_weights(self):
    y_prediction = self.predict(self.X)


    #calculating gradients
    dw = - (2 * (self.X.T).dot(self.Y - y_prediction)) / self.m
    db = - 2 * np.sum(self.Y - y_prediction)/self.m

    #updating weight
    self.w = self.w - self.learning_rate * dw
    self.b = self.b - self.learning_rate * db



  def predict(self,X):
    return X.dot(self.w) + self.b

"""#work flow
1. set learning rate and no of iterations  initiate random weight and bias
2. build linear regression equation (y =mx+c)
3.find y predict for given x value for the corresponding weight and bias
4. check the loss function for these parameters (diff btw y pred and true value)
5. update parameters value using gradient descent(new weight and bias)
6. repeat step 3,,4,5 till we get less loss fxn
"""

# using linear regression model for prediction
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

#data preprocessing
salary_data = pd.read_csv('/content/salary_data.csv')

salary_data.shape

salary_data.head()

salary_data.isnull().sum()   # if missisng any will tell

#splitting the feature and target
X = salary_data.iloc[:,:-1].values  # it removes last column
Y = salary_data.iloc[:,1].values   # it keep only ist column

print(X)

print(Y)

#splitting the dataset in training and test data
X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.33,random_state=2)

# training the linear regression model
model = LinearRegression(learning_rate=0.02,no_of_iterations=1000)

model.fit(X_train,Y_train)

print(model.w[0])
print(model.b)

#predicting the salary value for test data
test_data_prediction = model.predict(X_test)

print(test_data_prediction)

#visualizing the  predict values and actual values
plt.scatter(X_test,Y_test)
plt.plot(X_test,test_data_prediction,color='red',label='predicted value')
plt.legend()
plt.show()